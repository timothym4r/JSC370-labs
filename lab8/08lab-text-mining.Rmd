---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)
library(reshape2)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(fct_reorder(medical_specialty, n), n)) +
  geom_col(fill = "dodgerblue") +
  coord_flip() +
  theme_bw()
```

We have around 30 medical specialities in the dataset.

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarize(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_col(fill = "dodgerblue") +
  coord_flip() +
  theme_bw()

tokens |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.4, color="random-light", backgroundColor = "dodgerblue")
```
The top 20 most frequent words are stopwords. It does make sense because we use a lot of stopwords in sentences. Thus, we need to remove stopwords first to gain insights from the dataset.

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r}
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription, token="words") |>
  group_by(word) |>
  anti_join(stop_words, by="word") |>
  filter(!str_detect(word, "^[0-9]+$")) |>
  filter(word != "") |>
  filter(!word %in% c("mg", "mm", "noted")) |>
  summarize(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

tokens |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_col(fill = "dodgerblue") +
  coord_flip() +
  theme_bw()

tokens |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.3, color="random-light", backgroundColor = "dodgerblue")

```
Yes, I see more medical words/terms such as patient, procedure, pain, diagnosis, etc. It does give us a better idea of the text.


## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=FALSE}
stop_words2 <- c(stop_words$word, "mm", "mg", "noted")
sw_start <- paste0("^", paste(stop_words2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stop_words2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!grepl(sw_start, ngram, ignore.case = TRUE)) |>
  filter(!grepl(sw_end, ngram, ignore.case = TRUE)) |>
  filter(!grepl("[[:digit:]]+", ngram)) |>
  group_by(ngram) |>
  summarize(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |>
  head(20)

# Bar Plot of Top 20 Bi-Grams
tokens_bigram |>
  ggplot(aes(ngram, word_frequency)) +
  geom_col(fill = "dodgerblue") +
  coord_flip()+
  theme_bw()
```

When shifting from bi-grams to tri-grams, the results become more specific but less frequent. Bi-grams capture common medical phrases like "blood pressure" or "medical history," while tri-grams provide more context, revealing phrases like "high blood pressure" or "estimated blood loss." However, as phrases get longer, their frequency naturally decreases.

Stopword removal also changes. In bi-grams, stopwords typically appear at the start or end and are easy to filter. In tri-grams, they can appear in the middle (e.g., "history of patient"), making simple removal ineffective. A better approach is to remove stopwords before tokenization to avoid generating meaningless tri-grams.

Overall, tri-grams offer richer context but require refined stopword filtering.

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.


```{r}
library(stringr)
# e.g. patient, blood, preoperative...
tokens_bigram |>
  filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |> # finding pairs with "blood" then we remove the word "blood"
  mutate(word = str_remove(ngram, "blood"),
         word = str_remove_all(word, " ")) |>
  # sum ""xxx blood" and "blood xxx"
  group_by(word) |>
  head(20) |>
  ggplot(aes(reorder(word, word_frequency),
             word_frequency)) +
  geom_col(fill="dodgerblue") +
  theme_bw()
```


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r}
# mt_samples |>
#    unnest_tokens(word, transcription) |>
#    unnest_tokens(word, transcription token="words") |>
#    anti_join(stop_words, by="word") |>
#  # continue this
#    group_by(medical_specialty) |>
#    ...
stopwords2 <- c(stop_words$word, "mm", "mg", "noted")

top_words_by_specialty <- mt_samples |>
  unnest_tokens(word, transcription, token = "words") |>
  anti_join(tibble(word = stopwords2), by = "word") |>
  filter(!str_detect(word, "^[0-9]+$")) |>
  count(medical_specialty, word, sort = TRUE) |>
  group_by(medical_specialty) |>
  top_n(3) |>
  ungroup()

ggplot(top_words_by_specialty, aes(x = reorder_within(word, n, medical_specialty), y = n, fill = medical_specialty)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ medical_specialty, scales = "free_y") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "Top 3 Words in Each Medical Specialty",
       x = "Word",
       y = "Frequency") +
  theme_bw()
```
The most frequently used words in each medical specialty tend to reflect the focus of that specialty. For example, in cardiology, the most common words might include 'heart,' 'cardiac,' and 'pressure,' while in orthopedics, words like 'bone,' 'fracture,' and 'joint' might be dominant.

Looking at the entire dataset, the five most frequently used words across all specialties are 'patient,' 'history,' 'procedure,' 'pain,' and 'left.' These words are common in medical transcriptions regardless of specialty because they describe key aspects of medical records.

## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r eval=FALSE}

transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(stop_words, by="word") |>
  filter(!str_detect(word, "^[0-9]+$")) |>
  filter(!word %in% c("mg", "mm", "noted")) |>
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)  

transcripts_lda <- LDA(transcripts_dtm, k=5, control=list(seed=1234))

transcripts_top_terms <- tidy(transcripts_lda, matrix="beta") |>
  group_by(topic) |>
  top_n(10, beta) |>  # Select top 10 terms per topic
  ungroup() |>
  arrange(topic, desc(beta))

ggplot(transcripts_top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  theme_minimal() +
  labs(title = "Top Words in Each Topic", x = "Term", y = "Beta (Importance)")
```

