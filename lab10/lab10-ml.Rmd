---
title: "Lab 10 - Trees, Bagging, RF, Boosting, XGBoost"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(include  = T, echo = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with the `heart` dataset that you can download from [here](https://github.com/JSC370/JSC370-2025/blob/main/data/heart.csv)

# Deliverables

Questions 1-5 answered, pdf or html output uploaded to Quercus

### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages(c("rpart", "rpart.plot", "randomForest", "gbm", "xgboost"))
```

### Load packages and data
```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)

heart <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/refs/heads/main/data/heart/heart.csv") %>%
  mutate(
    AHD = 1 * (AHD == "Yes"),
    ChestPain = factor(ChestPain),
    Thal = factor(Thal)
  )

head(heart)
```


---


## Question 1: Trees
- Split the `heart` data into training and testing (70-30%)

```{r}
set.seed(42)

train = sample(1:nrow(heart), round(0.7* nrow(heart)))

heart_train = heart[train,]
heart_test = heart[-train,]

```

- Fit a classification tree using rpart, plot the full tree. We are trying to predict AHD. Set minsplit = 10, minbucket = 3, and do 10 cross validations.

```{r}
heart_tree <- rpart(
  AHD~., data = heart_train,
  method = "class",
  control = list(minsplit = 10, minbucket = 3, cp = 0, xval = 10)
)
rpart.plot(heart_tree)
```

- Plot the complexity parameter table for an rpart fit and find the optimal cp

```{r}
plotcp(heart_tree)
printcp(heart_tree)
```

```{r}
optimalcp = heart_tree$cptable[which.min(heart_tree$cptable[, "xerror"]), "CP"]# extract from cptable
optimalcp
```


- Prune the tree

```{r}
heart_tree_prune<-prune(heart_tree, cp = optimalcp)
rpart.plot(heart_tree_prune)
```

- Compute the test misclassification error

```{r}
heart_pred<-predict(heart_tree_prune, heart_test)
heart_pred = as.data.frame(heart_pred)
colnames(heart_pred) = c("No", "Yes")
heart_pred$AHD = ifelse(heart_pred$Yes >0.5,1,0)

```

```{r}
confmatrix_table = table(true = heart_test$AHD, predicted = heart_pred$AHD)
confmatrix_table
```

```{r}
miscla_error = ((confmatrix_table[1,2]+confmatrix_table[2,1])/nrow(heart_test))
miscla_error
```
We can see the misclassification error is 0.2307692.


- Fit the tree with the optimal complexity parameter to the full data (training + testing)

```{r}
heart_tree<-rpart(
  AHD~., data = heart,
  method = "class",
  control = list(minsplit = 10, minbucket = 3, cp = 0, xval = 10)
)

```

- Find the Out of Bag (OOB) error for tree

```{r}
heart_tree$cptable
```


---

## Question 2: Bagging, Random Forest

- Compare the performance of classification trees (above), bagging, random forests for predicting heart disease based on the ``heart`` data.

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 


- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r}
heart_bag<-randomForest(as.factor(AHD)~., data = heart_train,
                        mtry = 13, na.action = na.omit)



# oob error rate
sum(heart_bag$err.rate[, 1])

varImpPlot(heart_bag, cex.lab = 1.5,
           cex.axis = 2,
           cex = 1.3,
           n.var = 13,
           main = "",
           pch = 16,
           col = "blue4")
# importance()
```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.
---

# Question 3: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 



```{r}
heart_boost = gbm(AHD ~ ., data = heart_train, distribution = "bernoulli", n.trees = 3000, interaction.depth = 1, shrinkage = 0.01, cv.folds = 5, class.stratify.cv = TRUE)

plot(heart_boost$train.error, cex.lab =2, cex.axis = 2, col ="red4", type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")

lines(heart_boost$cv.error, col = "steelblue", lwd = 3)
```

---


## Question 4: Gradient Boosting

Evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction).  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

Note, boosting can overfit if the number of trees is too large. The shrinkage parameter controls the rate at which the boosting learns. Very small $\lambda$ can require using a very large number of trees to achieve good performance. Finally, interaction depth controls the interaction order of the boosted model. A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc. the default is 1.


- Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration and calculate the test MSE.

```{r}
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart_train, distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
```

```{r}
min(heart_boost$cv.error)
```
```{r}
heart_test <- heart_test %>%
  mutate(
    ChestPain = factor(ChestPain),
    Thal = factor(Thal),
    AHD = as.numeric(AHD == "Yes") # Convert AHD to binary numeric
  )
```


```{r}
yhat_boost = predict(heart_boost, newdata = heart_test, n.trees = 5000)
mse_boost = mean((yhat_boost-heart_test$AHD)^2)
print(mse_boost)
```


- Repeat the above using the same seed and ``n.trees=5000`` with the following 3 additional combination of parameters: a) ``shrinkage = 0.001``, ``interaction.depth = 2``; b) ``shrinkage = 0.01``, ``interaction.depth = 1``; c) ``shrinkage = 0.01``, ``interaction.depth = 2``.

```{r}
set.seed(301)
heart_boost = gbm(AHD ~ ., data = heart_train, distribution = "bernoulli", n.trees = 5000, interaction.depth = 1, shrinkage = 0.001, cv.folds = 10, class.stratify.cv = TRUE)

summary(heart_boost)
```

```{r}
pretty.gbm.tree(heart_boost)
plot(heart_boost$train.error, cex.lab =2, cex.axis = 2, col ="red", type = "l", lwd = 3, ylim = c(0, 1.5), ylab = "error")
lines(heart_boost$cv.error, col = "steelblue", lwd = 3)
```


## Question 5: Extreme Gradient Boosting

Train a XGBoost model with `xgboost` and perform a grid search for tuning the number of trees and the maximum depth of the tree. Also perform 10-fold cross-validation and determine the variable importance. Finally, compute the test MSE.

Tuning parameters
- max_depth: tree depth, larger makes model more complex and potentially overfit
- nrounds: number of boosting iterations
- eta: learning rate (shrinkage)
- gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be (simpler model)
- min_child_weight: controls the minimum number of samples in a leaf node before further splitting
- colsample_bytree: controls the fraction of features (variables) used to build each tree. Default is 1 which is all features

```{r}

train_control = trainControl(method = "cv", number = 10, search ="grid")

tune_grid<-  expand.grid(max_depth = c(1, 3, 5, 7), 
                        nrounds = (1:10)*50, 
                        eta = c(0.01,0.1,0.3), 
                        gamma = 0, 
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6 
                        )

heart_xgb<-caret::train(AHD ~., data = heart_train, method="xgbTree",
                        trControl = train_control,
                        tuneGrid = tune_grid,
                        na.action = na.exclude,
                        verbosity = 0)
```

- Compare the the performance of the different models and summarize

```{r}
varimp = varImp(heart_xgb, scale = FALSE)
plot(varimp)
yhat_xgb = predict(heart_xgb, newdata = heart_test)
mean((yhat_xgb-heart_test$AHD)^2)
caret::RMSE(heart_test$AHD, yhat_xgb)
```
The plot shows the variable importance from an XGBoost model predicting the AHD outcome (heart disease). The x-axis represents the relative importance of each feature in making predictions, while the y-axis lists the features. The most influential feature is Thalnormal, followed by Ca (number of major vessels colored by fluoroscopy), ExAng (exercise-induced angina), and MaxHR (maximum heart rate achieved). This suggests that the model relies heavily on these clinical indicators to make predictions. Features like Fbs (fasting blood sugar) and RestECG (resting electrocardiographic results) have relatively low importance, indicating they contribute less to the predictive power of the model.

